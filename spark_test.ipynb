{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Columbia EECS E6893 Big Data Analytics\n",
    "\n",
    "\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "import datetime\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "import subprocess\n",
    "import re\n",
    "from google.cloud import bigquery\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "import datetime\n",
    "\n",
    "def get_reset_seconds():\n",
    "    date_second = (datetime.datetime.now() + datetime.timedelta(seconds=30)).strftime(\"%Y-%m-%d\") + ' 00:00:00'\n",
    "\n",
    "    reset_ts = int(datetime.datetime.strptime(date_second, \"%Y-%m-%d %H:%M:%S\").timestamp())\n",
    "\n",
    "    return reset_ts - int(datetime.datetime.now().timestamp())\n",
    "\n",
    "TOPIC = \"topic{0}\"\n",
    "\n",
    "\n",
    "# parameter\n",
    "IP = 'localhost'    # ip port\n",
    "PORT = 9001     # port\n",
    "\n",
    "STREAMTIME = 60          # time that the streaming process runs\n",
    "\n",
    "WORD = ['percent', 'price', 'market', 'sale', 'rise',\n",
    "        'rate', 'share', 'stock', 'sell', 'increase']     #the words you should filter and do word count\n",
    "\n",
    "\n",
    "\n",
    "def stock_filter(line):\n",
    "\n",
    "    # TODO: insert your code here\n",
    "    \n",
    "\n",
    "    def isEnglish(s):\n",
    "        try:\n",
    "            s.encode(encoding='utf-8').decode('ascii')\n",
    "        except UnicodeDecodeError:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    def containStock(s):\n",
    "        for word in WORD:\n",
    "            if word in s:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "\n",
    "    line = line.map(lambda x: x.lower()).filter(lambda x: isEnglish(x)).filter(lambda x: containStock(x))\n",
    "    line = line.map(lambda x: (x, (datetime.datetime.now().timestamp() - start_time.value) // STREAMTIME))\n",
    "    return line\n",
    "    \n",
    "\n",
    "    \n",
    "def kafka_sink(line, producer):\n",
    "    producer = producer_sc.value\n",
    "    def send(tup):\n",
    "        topic_idx = tup[1]\n",
    "        msg = tup[0]\n",
    "        producer.send(TOPIC.format(\"X\"), msg)\n",
    "        return msg\n",
    "    \n",
    "    line = line.map(lambda x: send(x))\n",
    "    return line\n",
    "    \n",
    "\n",
    "def handler(message):\n",
    "    records = message.collect()\n",
    "    if len(records) <= 0: return\n",
    "    idx = int(records[0][1])\n",
    "    with open(\"./data/data{0}.txt\".format(idx), \"a\") as f:\n",
    "        for record in records:\n",
    "            idx = record[1]\n",
    "            msg = record[0]\n",
    "            f.write(msg + \"\\n\")\n",
    "#         producer.send('topicX', str(msg))\n",
    "#         producer.flush()\n",
    "\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVER = \"localhost:9092\"\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers = KAFKA_BOOTSTRAP_SERVER, \n",
    "                                             api_version=(0,11,5))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Spark settings\n",
    "    conf = SparkConf()\n",
    "    conf.setMaster('local[2]')\n",
    "    conf.setAppName(\"TwitterStreamApp\")\n",
    "\n",
    "    sc = SparkContext(conf=conf)\n",
    "#     sc.setLogLevel(\"ERROR\")\n",
    "    sc.setLogLevel(\"OFF\")\n",
    "    start_time = sc.broadcast(datetime.datetime.now().timestamp())\n",
    "    \n",
    "#     producer_sc = KafkaProducer(bootstrap_servers = KAFKA_BOOTSTRAP_SERVER, \n",
    "#                                              api_version=(0,11,5))\n",
    "#                                              value_serializer = lambda x: dump(x).encode(\"utf-8\")))\n",
    "    \n",
    "# \n",
    "\n",
    "    sql_context = SQLContext(sc)\n",
    "    ssc = StreamingContext(sc, 10)\n",
    "    # setting a checkpoint to allow RDD recovery\n",
    "    ssc.checkpoint(\"~/checkpoint_TwitterApp\")\n",
    "\n",
    "    # read data from port 9001\n",
    "    dataStream = ssc.socketTextStream(IP, PORT)\n",
    "\n",
    "\n",
    "    print(\"---------checkpoint{0}-----------\".format(0))\n",
    "\n",
    "\n",
    "\n",
    "    res = stock_filter(dataStream)\n",
    "#     res.pprint()\n",
    "    \n",
    "    res.foreachRDD(handler)\n",
    "#     res = kafka_sink(res)\n",
    "#     producer_sc.send(TOPIC.format(\"X\"), res)\n",
    "#     res.pprint()\n",
    "\n",
    "    ssc.start()\n",
    "\n",
    "    ssc.awaitTerminationOrTimeout(get_reset_seconds()) \n",
    "    time.sleep(STREAMTIME)\n",
    "\n",
    "    ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
